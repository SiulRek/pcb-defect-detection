{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defect Binary Classification\n",
    "![System Test Engineering Logo](../figures/logo_fhj_stm.jpg)<br>\n",
    "---\n",
    "\n",
    "**A Component my System Test Engineering Master's Thesis**  \n",
    "**FH JOANNEUM - University of Applied Sciences**\n",
    "\n",
    "---\n",
    "\n",
    "**Author:** Luis Kraker  \n",
    "**Supervisor:** DDr. Gudrun Schappacher-Tilp  \n",
    "**Date:** 28<sup>th</sup> March, 2024    \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import source.load_raw_data.kaggle_dataset as kaggle_dataset\n",
    "from source.model.helpers.image_classifier_visualizer import ImageClassifierVisualizer\n",
    "from source.model.helpers.image_classifiers_trainer import ImageClassifiersTrainer\n",
    "from source.image_preprocessing.image_preprocessor import ImagePreprocessor\n",
    "import source.image_preprocessing.preprocessing_steps as steps\n",
    "from source.load_raw_data.save_images_from_tf_dataset import save_images_from_tf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = os.path.join(os.path.curdir,  '..', '..', '..', '..', 'outputs', 'defect_binary_classification')\n",
    "IMAGES_DIR = os.path.join(OUTPUT_DIR, 'images')\n",
    "RESULTS_DIR = os.path.join(OUTPUT_DIR, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_defect_dataset = kaggle_dataset.get_tf_dataset_with_category_zero()\n",
    "defect_datasets = kaggle_dataset.get_tf_datasets_for_each_category()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_elements_in_datasets(datasets):\n",
    "    for category, dataset in datasets.items():\n",
    "        count = 0\n",
    "        for _ in dataset:\n",
    "            count += 1\n",
    "        print(f\"Category: {category}, Number of Images: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: MISSING_HOLE, Number of Images: 115\n",
      "Category: MOUSE_BITE, Number of Images: 115\n",
      "Category: OPEN_CIRCUIT, Number of Images: 116\n",
      "Category: SHORT, Number of Images: 116\n",
      "Category: SPUR, Number of Images: 115\n",
      "Category: SPURIOUS_COPPER, Number of Images: 116\n",
      "Category: NO DEFECT, Number of Images: 10\n"
     ]
    }
   ],
   "source": [
    "count_elements_in_datasets(defect_datasets)\n",
    "count_elements_in_datasets({\"NO DEFECT\": no_defect_dataset})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upsample in 'NO_DEFECT' dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_defect_dataset_repeated = no_defect_dataset.repeat(11)\n",
    "no_defect_dataset_partial = no_defect_dataset.take(5)\n",
    "no_defect_dataset_upsampled = no_defect_dataset_repeated.concatenate(no_defect_dataset_partial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: NO DEFECT, Number of Images: 115\n"
     ]
    }
   ],
   "source": [
    "count_elements_in_datasets({\"NO DEFECT\": no_defect_dataset_upsampled})  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate no_defect_dataset_upsampled with all defect_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_datasets = {}\n",
    "for category, dataset in defect_datasets.items():\n",
    "    concatenated_datasets[category] = dataset.concatenate(no_defect_dataset_upsampled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shuffle the Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "for category, dataset in concatenated_datasets.items():\n",
    "    concatenated_datasets[category] = dataset.shuffle(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binary Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_label_to_binary(image, label):\n",
    "    return image, tf.where(label == 0, 0, 1)\n",
    "\n",
    "binary_labeled_datasets = {}\n",
    "for category, dataset in concatenated_datasets.items():\n",
    "    binary_labeled_datasets[category] = dataset.map(map_label_to_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: MISSING_HOLE, Number of Images: 230\n",
      "Category: MOUSE_BITE, Number of Images: 230\n",
      "Category: OPEN_CIRCUIT, Number of Images: 231\n",
      "Category: SHORT, Number of Images: 231\n",
      "Category: SPUR, Number of Images: 230\n",
      "Category: SPURIOUS_COPPER, Number of Images: 231\n"
     ]
    }
   ],
   "source": [
    "count_elements_in_datasets(binary_labeled_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print Dimensions Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dimensions_statistics(datasets):\n",
    "    for category, dataset in datasets.items():\n",
    "        total_images = 0\n",
    "        sum_dims = tf.constant([0, 0], dtype=tf.int32)\n",
    "\n",
    "        for image, _ in dataset:\n",
    "            img_shape = tf.shape(image)\n",
    "            sum_dims += img_shape[:2]\n",
    "            total_images += 1\n",
    "\n",
    "        mean_dims = sum_dims / total_images\n",
    "\n",
    "        sum_squared_diff = tf.constant([0, 0], dtype=tf.float32)\n",
    "        for image, _ in dataset:\n",
    "            img_shape = tf.shape(image)\n",
    "            squared_diff = tf.square(tf.cast(img_shape[:2], tf.float32) - tf.cast(mean_dims, tf.float32))\n",
    "            sum_squared_diff += squared_diff\n",
    "\n",
    "        stddev_dims = tf.sqrt(sum_squared_diff / total_images)\n",
    "        aspect_ratio = mean_dims[0] / mean_dims[1]\n",
    "\n",
    "        print(f\"Category: {category}, Mean: {mean_dims.numpy()}, STDDev: {stddev_dims.numpy()}, Aspect Ratio: {aspect_ratio}\")\n",
    "\n",
    "print_dimensions_statistics(binary_labeled_datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)\n",
    "\n",
    "preprocessor = ImagePreprocessor()\n",
    "\n",
    "pipeline = [\n",
    "    steps.ShapeResizer(desired_shape=(20, 26)),\n",
    "    steps.RandomRotator(angle_range=(-3,3)),\n",
    "    steps.RandomFlipper(flip_direction='horizontal'),\n",
    "    steps.RandomFlipper(flip_direction='vertical'),\n",
    "]\n",
    "\n",
    "preprocessor.set_pipe(pipeline)\n",
    "processed_datasets = {}\n",
    "for category, dataset in binary_labeled_datasets.items():\n",
    "    processed_datasets[category] = preprocessor.process(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output images\n",
    "for category, dataset in processed_datasets.items():\n",
    "    save_images_from_tf_dataset(dataset, os.path.join(IMAGES_DIR, category), max_images=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.load_raw_data.save_images_from_tf_dataset import save_images_from_tf_dataset\n",
    "\n",
    "save_images_from_tf_dataset(processed_datasets['NO_DEFECT'].take(15), os.path.join(IMAGES_DIR, 'no_defect'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from source.utils.pcb_visualization import PCBVisualizerforTF as PCBVisualizer\n",
    "from source.load_raw_data.unpack_tf_dataset import unpack_tf_dataset\n",
    "\n",
    "no_defect_dataset = unpack_tf_dataset(processed_datasets['NO_DEFECT'])[0]\n",
    "visualizer = PCBVisualizer()\n",
    "visualizer.plot_images(no_defect_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = ImageClassifierVisualizer(categories)\n",
    "visualizer.plot_images(processed_datasets['NO_DEFECT'], n_cols=3, n_rows=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully split the datasets\n",
      "Category: MISSING_HOLE, Train Size: 161\n"
     ]
    }
   ],
   "source": [
    "train_datasets = {}\n",
    "validation_datasets = {}\n",
    "split_factor = 0.7\n",
    "\n",
    "for category, dataset in processed_datasets.items(): \n",
    "    dataset_length = dataset.reduce(0, lambda x, _: x + 1).numpy()\n",
    "    train_length = int(dataset_length * split_factor)\n",
    "    train_datasets[category] = dataset.take(train_length)\n",
    "    validation_datasets[category] = dataset.skip(train_length)\n",
    "\n",
    "print(\"Successfully split the datasets\")\n",
    "\n",
    "for category, dataset in train_datasets.items():\n",
    "    print(f\"Category: {category}, Train Size: {dataset.reduce(0, lambda x, _: x + 1).numpy()}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Batch Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_datasets = {category: dataset.batch(batch_size) for category, dataset in train_datasets.items()}\n",
    "validation_datasets = {category: dataset.batch(batch_size) for category, dataset in validation_datasets.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(20, 26, 3)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_datasets['MISSING_HOLE'],\n",
    "    validation_data=validation_datasets['MISSING_HOLE'],\n",
    "    epochs=5,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import source.model.helpers.image_classifiers_trainer as icv\n",
    "importlib.reload(icv)\n",
    "\n",
    "category_names = ['DEFECT', 'NO_DEFECT']\n",
    "group_names = list(binary_labeled_datasets.keys())\n",
    "\n",
    "trainer = icv.ImageClassifiersTrainer(category_names=category_names, group_names=group_names)\n",
    "trainer.load_model(model)\n",
    "trainer.fit_all(binary_labeled_datasets, epochs=5, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = trainer.plot_histories()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
